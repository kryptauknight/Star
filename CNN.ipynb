{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: snowballstemmer in /Users/thzaamoun/Library/Python/3.11/lib/python/site-packages (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sklearn in /Users/thzaamoun/Library/Python/3.11/lib/python/site-packages (0.0.post11)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /Users/thzaamoun/Library/Python/3.11/lib/python/site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: sympy in /Users/thzaamoun/Library/Python/3.11/lib/python/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/thzaamoun/Library/Python/3.11/lib/python/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/thzaamoun/Library/Python/3.11/lib/python/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/thzaamoun/Library/Python/3.11/lib/python/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2022.6)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement string (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for string\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement sys (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for sys\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting argparse\n",
      "  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Installing collected packages: argparse\n",
      "Successfully installed argparse-1.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sklearn in /Users/thzaamoun/Library/Python/3.11/lib/python/site-packages (0.0.post11)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gensim in /Users/thzaamoun/Library/Python/3.11/lib/python/site-packages (4.3.2)\n",
      "Requirement already satisfied: nltk in /Users/thzaamoun/Library/Python/3.11/lib/python/site-packages (3.8.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gensim) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/thzaamoun/Library/Python/3.11/lib/python/site-packages (from gensim) (1.11.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/thzaamoun/Library/Python/3.11/lib/python/site-packages (from gensim) (6.4.0)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /Users/thzaamoun/Library/Python/3.11/lib/python/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/thzaamoun/Library/Python/3.11/lib/python/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /Users/thzaamoun/Library/Python/3.11/lib/python/site-packages (from nltk) (4.66.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gensim in /Users/thzaamoun/Library/Python/3.11/lib/python/site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gensim) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/thzaamoun/Library/Python/3.11/lib/python/site-packages (from gensim) (1.11.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/thzaamoun/Library/Python/3.11/lib/python/site-packages (from gensim) (6.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: keras in /Users/thzaamoun/Library/Python/3.11/lib/python/site-packages (2.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in /Users/thzaamoun/Library/Python/3.11/lib/python/site-packages (3.8.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (1.0.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/thzaamoun/Library/Python/3.11/lib/python/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/thzaamoun/Library/Python/3.11/lib/python/site-packages (from matplotlib) (4.44.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/thzaamoun/Library/Python/3.11/lib/python/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (22.0)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/thzaamoun/Library/Python/3.11/lib/python/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: stanza in /Users/thzaamoun/Library/Python/3.11/lib/python/site-packages (1.6.1)\n",
      "Requirement already satisfied: emoji in /Users/thzaamoun/Library/Python/3.11/lib/python/site-packages (from stanza) (2.8.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from stanza) (1.23.5)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from stanza) (4.21.11)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from stanza) (2.28.1)\n",
      "Requirement already satisfied: torch>=1.3.0 in /Users/thzaamoun/Library/Python/3.11/lib/python/site-packages (from stanza) (2.1.0)\n",
      "Requirement already satisfied: tqdm in /Users/thzaamoun/Library/Python/3.11/lib/python/site-packages (from stanza) (4.66.1)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.3.0->stanza) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.3.0->stanza) (4.4.0)\n",
      "Requirement already satisfied: sympy in /Users/thzaamoun/Library/Python/3.11/lib/python/site-packages (from torch>=1.3.0->stanza) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/thzaamoun/Library/Python/3.11/lib/python/site-packages (from torch>=1.3.0->stanza) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.3.0->stanza) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/thzaamoun/Library/Python/3.11/lib/python/site-packages (from torch>=1.3.0->stanza) (2023.10.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->stanza) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->stanza) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->stanza) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->stanza) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/thzaamoun/Library/Python/3.11/lib/python/site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install snowballstemmer\n",
    "%pip install torch\n",
    "%pip install pandas\n",
    "%pip install string\n",
    "%pip install sys\n",
    "%pip install argparse\n",
    "%pip install sklearn\n",
    "%pip install gensim\n",
    "%pip install keras\n",
    "%pip install matplotlib\n",
    "%pip install stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Lib Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Dataset + Shuffling + Sampling + Setting the Pandas Series for Simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>ممتاز نوعا ما . النظافة والموقع والتجهيز والشا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>أحد أسباب نجاح الإمارات أن كل شخص في هذه الدول...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>هادفة .. وقوية. تنقلك من صخب شوارع القاهرة الى...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>خلصنا .. مبدئيا اللي مستني ابهار زي الفيل الاز...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>ياسات جلوريا جزء لا يتجزأ من دبي . فندق متكامل...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "0  Positive  ممتاز نوعا ما . النظافة والموقع والتجهيز والشا...\n",
       "1  Positive  أحد أسباب نجاح الإمارات أن كل شخص في هذه الدول...\n",
       "2  Positive  هادفة .. وقوية. تنقلك من صخب شوارع القاهرة الى...\n",
       "3  Positive  خلصنا .. مبدئيا اللي مستني ابهار زي الفيل الاز...\n",
       "4  Positive  ياسات جلوريا جزء لا يتجزأ من دبي . فندق متكامل..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/thzaamoun/Desktop/Academia/Fall 23/NLP/Assignment 2/VSCODE/ar_reviews_100k.tsv',sep='\\t')\n",
    "reviews = df['text']  # Replace with your actual column name\n",
    "labels = df['label']    # Replace with your actual label column name\n",
    "# First, shuffle the DataFrame\n",
    "df_shuffled = shuffle(df, random_state=42)\n",
    "\n",
    "#Sample 20% of the dataframe\n",
    "df_sampled = df_shuffled.sample(frac=1, random_state=42)  # frac=0.2 means 20%\n",
    "\n",
    "# Then, split the features and labels\n",
    "X = df_sampled['text'].values\n",
    "y = df_sampled['label'].values\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # No need to remove diacretics since not there\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text\n",
    "\n",
    "reviews = reviews.apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowballstemmer import stemmer\n",
    "\n",
    "# Arabic stemmer\n",
    "ar_stemmer = stemmer(\"arabic\")\n",
    "\n",
    "# Stems all words in given text\n",
    "def stem_text(text):\n",
    "    words = text.split()\n",
    "    stemmed_words = [ar_stemmer.stemWord(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "# Apply to all entries in the pd Series\n",
    "reviews = reviews.apply(stem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     ممتاز نوع ما نظاف والموقع والتجهيز والشاطيء مطعم\n",
       "1    احد اسباب نجاح امار ان كل شخص في هذه دول يعشق ...\n",
       "2    هادف وقو تنقل من صخب شوارع قاهر الي هدوء جبال ...\n",
       "3    خلص مبديي الل مست ابهار زي فيل ازرق ميقراش احس...\n",
       "4    ياسا جلوري جزء لا يتجزء من دب ندق متكامل خدم م...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Vocab (PAD + UNK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab(sentences, min_word_freq):\n",
    "    # Count word frequencies\n",
    "    word_freq = Counter(word for sentence in sentences for word in sentence)\n",
    "    \n",
    "    # Remove words below a certain frequency\n",
    "    word_freq = {word: freq for word, freq in word_freq.items() if freq >= min_word_freq}\n",
    "\n",
    "    # Build the vocabulary\n",
    "    vocab = {word: index + 2 for index, word in enumerate(word_freq)} # +2 for special tokens\n",
    "    vocab['<PAD>'] = 0  # To Pad\n",
    "    vocab['<UNK>'] = 1  # Add an unknown token\n",
    "\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(reviews, min_word_freq=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-To-Sequence Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequence(sentences, vocab):\n",
    "    sequences = []\n",
    "    for sentence in sentences:\n",
    "        #UNK for unknown tokens = out-of-vocab\n",
    "        seq = [vocab.get(word, vocab['<UNK>']) for word in sentence]\n",
    "        sequences.append(seq)\n",
    "    return sequences\n",
    "\n",
    "sequences = text_to_sequence(reviews, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 17:12:46.929169: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_seq_length = 100\n",
    "sequences_padded = pad_sequences(sequences, maxlen=max_seq_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sequences_padded, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int32\n",
      "int32\n",
      "object\n",
      "object\n"
     ]
    }
   ],
   "source": [
    "print(X_train.dtype)\n",
    "print(X_test.dtype)\n",
    "print(y_train.dtype)\n",
    "print(y_test.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Sequence-Lists to Numpy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_seq_length = 100\n",
    "X_train_padded = pad_sequences(X_train, maxlen=max_seq_length, padding='post', dtype='int64')\n",
    "X_test_padded = pad_sequences(X_test, maxlen=max_seq_length, padding='post', dtype='int64')\n",
    "\n",
    "# -> To PyTorch tensors\n",
    "train_data = torch.tensor(X_train_padded, dtype=torch.long)\n",
    "test_data = torch.tensor(X_test_padded, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thzaamoun/Library/Python/3.11/lib/python/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch\n",
    "\n",
    "# Check for unusual DTs\n",
    "for item in np.concatenate([y_train.flatten(), y_test.flatten()]):\n",
    "    if isinstance(item, type(...)) or not isinstance(item, (str, int, float)):\n",
    "        print(\"Unusual item found:\", item)\n",
    "\n",
    "# Standardize Ellipses\n",
    "y_train = np.array(['Unknown' if isinstance(item, type(...)) else item for item in y_train.flatten()])\n",
    "y_test = np.array(['Unknown' if isinstance(item, type(...)) else item for item in y_test.flatten()])\n",
    "\n",
    "# Reshape to 2D after replacement\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the training data\n",
    "y_train_encoded = onehot_encoder.fit_transform(y_train)\n",
    "\n",
    "# Transform the test data using the same encoder\n",
    "y_test_encoded = onehot_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting to Pytorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "train_data = torch.tensor(X_train, dtype=torch.long)\n",
    "test_data = torch.tensor(X_test, dtype=torch.long)\n",
    "train_labels = torch.tensor(y_train_encoded, dtype=torch.float32)\n",
    "test_labels = torch.tensor(y_test_encoded, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: torch.Size([79999, 100])\n",
      "Train labels shape: torch.Size([79999, 3])\n",
      "Test data shape: torch.Size([20000, 100])\n",
      "Test labels shape: torch.Size([20000, 3])\n"
     ]
    }
   ],
   "source": [
    "# Check Dimensions of the datasets\n",
    "print(\"Train data shape:\", train_data.shape)\n",
    "print(\"Train labels shape:\", train_labels.shape)\n",
    "print(\"Test data shape:\", test_data.shape)\n",
    "print(\"Test labels shape:\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape: torch.Size([79999, 100])\n",
      "train_labels shape: torch.Size([79999, 3])\n",
      "test_data shape: torch.Size([20000, 100])\n",
      "test_labels shape: torch.Size([20000, 3])\n",
      "Adjusted train_data shape: torch.Size([79999, 100])\n",
      "Adjusted test_data shape: torch.Size([20000, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Check shapes\n",
    "print(\"train_data shape:\", train_data.shape)\n",
    "print(\"train_labels shape:\", train_labels.shape)\n",
    "print(\"test_data shape:\", test_data.shape)\n",
    "print(\"test_labels shape:\", test_labels.shape)\n",
    "\n",
    "# Correcting mismatch\n",
    "# Trim train_data to match size of train_labels\n",
    "if train_data.shape[0] > train_labels.shape[0]:\n",
    "    train_data = train_data[:train_labels.shape[0]]\n",
    "\n",
    "# Trim test_data to match the size of test_labels\n",
    "if test_data.shape[0] > test_labels.shape[0]:\n",
    "    test_data = test_data[:test_labels.shape[0]]\n",
    "\n",
    "# Verify the adjustment\n",
    "print(\"Adjusted train_data shape:\", train_data.shape)\n",
    "print(\"Adjusted test_data shape:\", test_data.shape)\n",
    "\n",
    "# Recreate the datasets and loaders\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_data, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating DataLoaders for efficient training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_data, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = np.load('/Users/thzaamoun/Desktop/Academia/Fall 23/NLP/Assignment 2/VSCODE/full_grams_cbow_300_wiki/full_grams_cbow_300_wiki.mdl.wv.vectors.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU Model Class-Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, num_filters, filter_sizes, num_classes, dropout):\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=fs)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(len(filter_sizes) * num_filters, num_classes)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = [batch size, sent len]\n",
    "        x = self.embedding(x)\n",
    "        # x = [batch size, sent len, emb dim]\n",
    "\n",
    "        # Convolution and pooling\n",
    "        x = x.permute(0, 2, 1)  # reshape for conv layers\n",
    "        # x = [batch size, emb dim, sent len]\n",
    "\n",
    "        x = [F.relu(conv(x)) for conv in self.convs]  # apply convolution and ReLU\n",
    "        x = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in x]  # apply max pooling\n",
    "\n",
    "        # Concatenate the convolutional layer outputs\n",
    "        x = torch.cat(x, dim=1)\n",
    "\n",
    "        # Dropout and fully connected layer\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Instanciation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_embeddings = 114000  # Size of your vocabulary\n",
    "embedding_dim = 300     # Dimension of word embeddings\n",
    "num_filters = 100       # Number of filters per convolutional layer\n",
    "filter_sizes = [3, 4, 5]  # Sizes of the convolutional layer filters\n",
    "num_classes = 3         # Number of output classes\n",
    "dropout = 0.5           # Dropout rate\n",
    "\n",
    "model = CNNModel(num_embeddings, embedding_dim, num_filters, filter_sizes, num_classes, dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Integrity of input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum index in train_data: tensor(57246)\n"
     ]
    }
   ],
   "source": [
    "# Check the maximum index in your input data\n",
    "max_index = train_data.max()\n",
    "print(\"Maximum index in train_data:\", max_index)\n",
    "\n",
    "# Ensure that this max index is less than num_embeddings\n",
    "assert max_index < model.embedding.num_embeddings, \"Index out of range. Increase num_embeddings in your model.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mode of Running The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNModel(\n",
       "  (embedding): Embedding(114000, 300)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv1d(300, 100, kernel_size=(3,), stride=(1,))\n",
       "    (1): Conv1d(300, 100, kernel_size=(4,), stride=(1,))\n",
       "    (2): Conv1d(300, 100, kernel_size=(5,), stride=(1,))\n",
       "  )\n",
       "  (fc): Linear(in_features=300, out_features=3, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Loss Function & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique label indices in the training dataset: {0, 1, 2}\n"
     ]
    }
   ],
   "source": [
    "unique_labels = set()\n",
    "for _, labels in train_loader:\n",
    "    labels = torch.argmax(labels, dim=1)\n",
    "    unique_labels.update(labels.tolist())\n",
    "print(\"Unique label indices in the training dataset:\", unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10  # Number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for inputs, labels in train_loader:\n",
    "        # Convert one-hot encoded labels to class indices if necessary\n",
    "        if labels.dim() > 1 and labels.size(1) > 1:\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "\n",
    "        # Filter out instances with label '3'\n",
    "        valid_indices = labels != 3\n",
    "        inputs = inputs[valid_indices]\n",
    "        labels = labels[valid_indices]\n",
    "\n",
    "        # Check if label '3' still exists in the batch\n",
    "        if labels.max() >= 3:\n",
    "            print(\"Error: Label '3' found in batch after filtering\")\n",
    "            continue  # Skip this batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss and backpropagate\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "# 3 features for the 3 classes\n",
    "assert model.fc.out_features == 3, \"Model's output layer should have 3 features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if labels.dim() > 1 and labels.size(1) > 1:\n",
    "    labels = torch.argmax(labels, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.505%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # If labels are one-hot encoded or in a 2D format, convert them to 1D class indices\n",
    "        if labels.dim() > 1 and labels.size(1) > 1:\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy: {100 * correct / total}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'cnn_model_state_dict.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
